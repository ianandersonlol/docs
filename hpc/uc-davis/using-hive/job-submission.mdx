---
title: Job Submission on HIVE
description: HIVE-specific job submission examples and interactive sessions
---

This page covers HIVE-specific job submission details. For general SLURM fundamentals (commands, script structure, time formats), see [Job Submission Basics](/hpc/job-submission).

## HIVE Example Scripts

These examples use HIVE partitions (`high`, `low`, `gpu-a100`, `gpu-a6000`) and available modules. Replace `your_account` with your actual account (e.g., `jbsiegelgrp`, `publicgrp`).

<Tabs>
  <Tab title="Basic CPU">
    Standard single-node CPU job:

    ```bash
    #!/bin/bash
    #SBATCH --job-name=cpu_job
    #SBATCH --partition=high
    #SBATCH --account=your_account
    #SBATCH --time=24:00:00
    #SBATCH --nodes=1
    #SBATCH --ntasks=1
    #SBATCH --cpus-per-task=16
    #SBATCH --mem=64G
    #SBATCH --output=%x_%j.out

    module load conda/latest
    conda activate myenv

    python my_script.py
    ```
  </Tab>
  <Tab title="OpenMP Parallel">
    Multi-threaded job using OpenMP:

    ```bash
    #!/bin/bash
    #SBATCH --job-name=openmp_job
    #SBATCH --partition=high
    #SBATCH --account=your_account
    #SBATCH --time=12:00:00
    #SBATCH --nodes=1
    #SBATCH --ntasks=1
    #SBATCH --cpus-per-task=32
    #SBATCH --mem=128G
    #SBATCH --output=%x_%j.out

    export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

    ./my_openmp_program
    ```
  </Tab>
  <Tab title="GPU (PyTorch)">
    GPU job for deep learning:

    ```bash
    #!/bin/bash
    #SBATCH --job-name=gpu_training
    #SBATCH --partition=gpu-a100
    #SBATCH --account=your_account
    #SBATCH --time=2-00:00:00
    #SBATCH --nodes=1
    #SBATCH --ntasks=1
    #SBATCH --cpus-per-task=16
    #SBATCH --mem=64G
    #SBATCH --gres=gpu:a100:1
    #SBATCH --output=%x_%j.out

    # Load CUDA (may not be needed if using conda PyTorch)
    module load cuda/12.6.2
    module load conda/pytorch/2.5.1

    python train_model.py
    ```

    <Note>
      For GPU accounts, see [Siegel Lab Guide](/hpc/uc-davis/using-hive/siegel-lab) or use `publicgrp`.
    </Note>
  </Tab>
  <Tab title="Array Job">
    Run many similar tasks with different inputs:

    ```bash
    #!/bin/bash
    #SBATCH --job-name=array_job
    #SBATCH --partition=high
    #SBATCH --account=your_account
    #SBATCH --time=2:00:00
    #SBATCH --array=1-100
    #SBATCH --ntasks=1
    #SBATCH --cpus-per-task=4
    #SBATCH --mem=16G
    #SBATCH --output=array_%A_%a.out

    echo "Processing task $SLURM_ARRAY_TASK_ID"
    python process_file.py --index $SLURM_ARRAY_TASK_ID
    ```

    **Max array size on HIVE:** 50,000 tasks
  </Tab>
  <Tab title="MPI">
    Multi-node MPI job:

    ```bash
    #!/bin/bash
    #SBATCH --job-name=mpi_job
    #SBATCH --partition=high
    #SBATCH --account=your_account
    #SBATCH --time=8:00:00
    #SBATCH --nodes=2
    #SBATCH --ntasks-per-node=64
    #SBATCH --mem=256G
    #SBATCH --output=%x_%j.out

    module load openmpi/5.0.5

    srun ./my_mpi_program
    ```
  </Tab>
  <Tab title="GROMACS MD">
    Molecular dynamics simulation:

    ```bash
    #!/bin/bash
    #SBATCH --job-name=gromacs_md
    #SBATCH --partition=high
    #SBATCH --account=your_account
    #SBATCH --time=7-00:00:00
    #SBATCH --nodes=1
    #SBATCH --ntasks=1
    #SBATCH --cpus-per-task=64
    #SBATCH --mem=128G
    #SBATCH --output=%x_%j.out

    module load gromacs/2023.4

    gmx mdrun -deffnm production -ntomp $SLURM_CPUS_PER_TASK
    ```
  </Tab>
</Tabs>

## Interactive Sessions

When you SSH into HIVE, you land on the **login node** (also called the head node). This shared node is only for editing files, submitting jobs, and light tasks. Running computationally intensive work on the login node slows it down for everyone.

<Warning>
  **Do not run jobs on the login node.** It affects all users and you'll receive sad emails from cluster admins (or fellow users) asking you to stop.
</Warning>

Instead, use **interactive sessions** to get a shell on a compute node. Once you're on a compute node, you can treat it like a regular computerâ€”run Python scripts, test code, install packages, debug interactively, whatever you need. The difference is you have dedicated resources that don't impact other users.

### Common Configurations

<Tabs>
  <Tab title="CPU Sessions">
    **High priority** (won't be preempted):
    ```bash
    srun -p high -c 8 --mem=16G -t 1-00:00:00 --pty bash
    ```

    **Low priority** (can be preempted, but often faster to start):
    ```bash
    srun -p low -c 16 --mem=32G -t 1-00:00:00 --requeue --pty bash
    ```
  </Tab>
  <Tab title="GPU Sessions">
    **High priority GPU**:
    ```bash
    srun -p gpu-a100 --account=your_account -c 8 --mem=16G --gres=gpu:1 -t 1-00:00:00 --pty bash
    ```

    **Low priority GPU** (can be preempted):
    ```bash
    srun -p gpu-a100 --account=your_account -c 8 --mem=16G --gres=gpu:1 -t 1-00:00:00 --requeue --pty bash
    ```

    <Note>
      Replace `your_account` with your lab/department account or `publicgrp`.
    </Note>
  </Tab>
</Tabs>

### Shell Aliases

These commands are long. Add aliases to your `~/.bashrc` or `~/.zshrc` for commands you use frequently:

```bash
# CPU interactive session - use your own lab/department account or publicgrp
alias cacao='srun -p high -A jbsiegelgrp -c 8 --mem=16G -t 1-00:00:00 --pty bash'

# GPU interactive session - use low partition or your own lab/department's resources
alias kakawa='srun -p gpu-a100 -A genome-center-grp -c 8 --mem=16G --gres=gpu:1 -t 1-00:00:00 --pty bash'
```

Then just type `cacao` or `kakawa` to start a session.

### Using salloc for Persistent Allocation

If you need to run multiple commands within an allocation:

```bash
# Allocate resources first
salloc -p high -A your_account -t 8:00:00 -c 16 --mem=64G

# Now you have an allocation - run commands with srun
srun python my_script.py
srun ./another_program

# Exit to release the allocation
exit
```

<Warning>
  Interactive allocations consume resources even when idle. Don't forget to `exit` when you're done to release them for other users.
</Warning>
