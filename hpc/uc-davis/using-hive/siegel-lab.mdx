---
title: Siegel Lab Guide
description: HIVE-specific information for Siegel Lab (jbsiegelgrp) members
---

This page contains HIVE configuration and best practices specific to members of the Siegel Lab (`jbsiegelgrp`). If you're not a member of the Siegel Lab, the general [Using HIVE](/hpc/uc-davis/using-hive) guides still apply to you.

<Note>
  This information is maintained by Siegel Lab members and reflects our specific account allocations and workflows.
</Note>

## Account and QOS Limits

The `jbsiegelgrp` account has the following resource limits:

| QOS | Max CPUs | Max Memory | Max GPUs |
|-----|----------|------------|----------|
| jbsiegelgrp-high-qos | 128 | 1 TB | 0 |

<Warning>
  The jbsiegelgrp account does **not** have direct GPU allocation. You must use alternative accounts for GPU jobs.
</Warning>

## Submitting Jobs

### CPU Jobs

For standard CPU jobs, use the lab account:

```bash
sbatch -A jbsiegelgrp job.sh
```

Or in your job script:

```bash
#SBATCH --account=jbsiegelgrp
```

### GPU Jobs

Since `jbsiegelgrp` has no GPU allocation, you have two options:

<Tabs>
  <Tab title="publicgrp (Recommended)">
    Use the public account with the `low` partition for GPU access:

    ```bash
    sbatch -A publicgrp -p low --gres=gpu:1 job.sh
    ```

    <Warning>
      The `publicgrp` account cannot access the `gpu-a100` partition. You must use the `low` partition for GPU jobs, which means your jobs can be preempted (cancelled and requeued) by higher-priority jobs.
    </Warning>

    **Specifying GPU type:**

    Two GPU types are available through the `low` partition:
    - **A100s** - Newer, faster GPUs
    - **A6000s** - Also available

    You can request a specific type or any available GPU:
    ```bash
    # Request any available GPU
    --gres=gpu:1

    # Request specifically an A100
    --gres=gpu:a100:1

    # Request specifically an A6000
    --gres=gpu:a6000:1
    ```

    **publicgrp limits:**
    - 5 GPUs
    - 128 CPUs
    - 2 TB memory

    <Note>
      This is the easiest option and doesn't require special access approval.
    </Note>
  </Tab>
  <Tab title="genome-center-grp">
    For higher priority GPU access, use the Genome Center account:

    ```bash
    sbatch -A genome-center-grp -p gpu-a100 --gres=gpu:a100:1 job.sh
    ```

    **genome-center-grp limits:**
    - 8 GPUs
    - Higher priority than publicgrp

    <Warning>
      You must request access to this account. Only Ian Korf can approve access, and as a PI he's very busy. Only request this if you genuinely need priority GPU access, and please be patient with the approval process.
    </Warning>
  </Tab>
</Tabs>

## Lab Storage

### Directory Structure

Lab storage is located at `/quobyte/jbsiegelgrp/`:

```
/quobyte/jbsiegelgrp/
├── [member_name]/     # Personal directories
├── databases/         # Shared databases
└── software/          # Shared software/tools
```

Each lab member typically creates their own subdirectory for personal projects.

### Best Practices

<Steps>
  <Step title="Store large files in lab storage">
    Keep datasets, simulation outputs, and large results in `/quobyte/jbsiegelgrp/your_username/`, not in your home directory.
  </Step>
  <Step title="Check shared resources first">
    Before downloading databases or installing software, check if they're already available:
    ```bash
    ls /quobyte/jbsiegelgrp/databases/
    ls /quobyte/jbsiegelgrp/software/
    ```
  </Step>
  <Step title="Coordinate large downloads">
    If you need to download a large database that others might use, consider putting it in the shared `databases/` directory and letting the lab know.
  </Step>
</Steps>

## Monitoring Lab Usage

### View Current Lab Jobs

```bash
squeue -A jbsiegelgrp
```

### View Lab's Historical Usage

```bash
sreport cluster AccountUtilizationByUser Account=jbsiegelgrp Start=2026-01-01
```

This helps understand how lab resources are being used and can inform discussions about resource allocation.

## Efficient Resource Usage

Since the lab has limited concurrent CPU allocation (128 CPUs max), being efficient helps everyone:

<AccordionGroup>
  <Accordion title="Don't over-request resources">
    Requesting more than you need:
    - Slows down your queue time
    - Blocks other lab members' jobs

    Check actual usage with `sacct` after jobs complete and adjust future requests.
  </Accordion>

  <Accordion title="Use array jobs for parameter sweeps">
    Instead of submitting 100 separate jobs:
    ```bash
    #SBATCH --array=1-100
    ```

    This is more efficient for the scheduler and easier to manage.
  </Accordion>

  <Accordion title="Consider the low partition">
    For jobs that can handle restarts, the `low` partition may have faster queue times:
    ```bash
    #SBATCH --partition=low
    ```

    Just make sure your code can checkpoint and resume.
  </Accordion>

  <Accordion title="Cancel jobs you don't need">
    If you realize a job has a bug or you no longer need the results, cancel it:
    ```bash
    scancel JOBID
    ```

    This frees resources for other lab members.
  </Accordion>
</AccordionGroup>

## Getting Help

### Lab Resources

- **Siegel Lab Discord:** Ask in the drylab channel
- **Lab members:** Don't hesitate to ask more experienced members

<Tip>
  We're a very independent lab, which means getting help from other members is really important. ChatGPT and Claude are great, but they don't know the exact inner workings of HIVE. It can feel hard or annoying to ask people for help, but it's genuinely part of the lab culture and will help you in the long run.
</Tip>

### Cluster Support

- **HPCCF Documentation:** [https://hpc.ucdavis.edu/](https://hpc.ucdavis.edu/)
- **Submit a ticket:** hpc-help@ucdavis.edu
- **DataLab Office Hours:** Check the [DataLab website](https://datalab.ucdavis.edu/) for schedule

<Tip>
  You can also ask me (Ian) directly—I'm happy to help with anything beyond the VERY basics. That said, if it's something you could have easily Googled, it will make me sad. For more complex problems, don't hesitate to reach out.
</Tip>
